{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#reference https://docs.openvino.ai/2023.0/notebooks/237-segment-anything-with-output.html\nimport warnings\nfrom pathlib import Path\nimport torch\nfrom openvino.tools import mo\nfrom openvino.runtime import serialize, Core\n\nfrom segment_anything import sam_model_registry, SamPredictor\nmodel_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\ncheckpoint = \"sam_vit_b_01ec64.pth\"\nmodel_type = \"vit_b\"\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\ncore = Core()\n\nov_encoder_path = Path(\"sam_image_encoder.xml\")\nonnx_encoder_path = ov_encoder_path.with_suffix(\".onnx\")\nif not ov_encoder_path.exists():\n    if not onnx_encoder_path.exists():\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            torch.onnx.export(sam.image_encoder, torch.zeros(1,3,1024,1024), onnx_encoder_path)\n    ov_encoder_model = mo.convert_model(onnx_encoder_path, compress_to_fp16=True)\n    serialize(ov_encoder_model, str(ov_encoder_path))\n    print(\"sam_image_encoder IR convetion done!\")\nelse:\n    ov_encoder_model = core.read_model(ov_encoder_path)\n    print(\"sam_image_encoder already exists, skip convertion\")\n\n\n#decoder\nfrom typing import Tuple\nfrom segment_anything.utils.amg import calculate_stability_score\n\nclass SamONNXModel(torch.nn.Module):\n    def __init__(\n        self,\n        model,\n        return_single_mask: bool,\n        use_stability_score: bool = False,\n        return_extra_metrics: bool = False,\n    ) -> None:\n        super().__init__()\n        self.mask_decoder = model.mask_decoder\n        self.model = model\n        self.img_size = model.image_encoder.img_size\n        self.return_single_mask = return_single_mask\n        self.use_stability_score = use_stability_score\n        self.stability_score_offset = 1.0\n        self.return_extra_metrics = return_extra_metrics\n\n    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n        point_coords = point_coords + 0.5\n        point_coords = point_coords / self.img_size\n        point_embedding = self.model.prompt_encoder.pe_layer._pe_encoding(point_coords)\n        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n\n        point_embedding = point_embedding * (point_labels != -1)\n        point_embedding = point_embedding + self.model.prompt_encoder.not_a_point_embed.weight * (\n            point_labels == -1\n        )\n\n        for i in range(self.model.prompt_encoder.num_point_embeddings):\n            point_embedding = point_embedding + self.model.prompt_encoder.point_embeddings[\n                i\n            ].weight * (point_labels == i)\n\n        return point_embedding\n\n    def t_embed_masks(self, input_mask: torch.Tensor) -> torch.Tensor:\n        mask_embedding = self.model.prompt_encoder.mask_downscaling(input_mask)\n        return mask_embedding\n\n    def mask_postprocessing(self, masks: torch.Tensor) -> torch.Tensor:\n        masks = torch.nn.functional.interpolate(\n            masks,\n            size=(self.img_size, self.img_size),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n        return masks\n\n    def select_masks(\n        self, masks: torch.Tensor, iou_preds: torch.Tensor, num_points: int\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Determine if we should return the multiclick mask or not from the number of points.\n        # The reweighting is used to avoid control flow.\n        score_reweight = torch.tensor(\n            [[1000] + [0] * (self.model.mask_decoder.num_mask_tokens - 1)]\n        ).to(iou_preds.device)\n        score = iou_preds + (num_points - 2.5) * score_reweight\n        best_idx = torch.argmax(score, dim=1)\n        masks = masks[torch.arange(masks.shape[0]), best_idx, :, :].unsqueeze(1)\n        iou_preds = iou_preds[torch.arange(masks.shape[0]), best_idx].unsqueeze(1)\n\n        return masks, iou_preds\n\n    @torch.no_grad()\n    def forward(\n        self,\n        image_embeddings: torch.Tensor,\n        point_coords: torch.Tensor,\n        point_labels: torch.Tensor,\n        mask_input: torch.Tensor = None,\n    ):\n        sparse_embedding = self._embed_points(point_coords, point_labels)\n        if mask_input is None:\n            dense_embedding = self.model.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n                point_coords.shape[0], -1, image_embeddings.shape[0], 64\n            )\n        else:\n            dense_embedding = self._embed_masks(mask_input)\n\n        masks, scores = self.model.mask_decoder.predict_masks(\n            image_embeddings=image_embeddings,\n            image_pe=self.model.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embedding,\n            dense_prompt_embeddings=dense_embedding,\n        )\n\n        if self.use_stability_score:\n            scores = calculate_stability_score(\n                masks, self.model.mask_threshold, self.stability_score_offset\n            )\n\n        if self.return_single_mask:\n            masks, scores = self.select_masks(masks, scores, point_coords.shape[1])\n\n        upscaled_masks = self.mask_postprocessing(masks)\n\n        if self.return_extra_metrics:\n            stability_scores = calculate_stability_score(\n                upscaled_masks, self.model.mask_threshold, self.stability_score_offset\n            )\n            areas = (upscaled_masks > self.model.mask_threshold).sum(-1).sum(-1)\n            return upscaled_masks, scores, stability_scores, areas, masks\n\n        return upscaled_masks, scores\n\nov_model_path = Path(\"sam_mask_predictor.xml\")\nif not ov_model_path.exists():\n    onnx_model_path = ov_model_path.with_suffix('.onnx')\n    if not onnx_model_path.exists():\n        onnx_model = SamONNXModel(sam, return_single_mask=True)\n        dynamic_axes = {\n            \"point_coords\": {0: \"batch_size\", 1: \"num_points\"},\n            \"point_labels\": {0: \"batch_size\", 1: \"num_points\"},\n        }\n\n        embed_dim = sam.prompt_encoder.embed_dim\n        embed_size = sam.prompt_encoder.image_embedding_size\n        dummy_inputs = {\n            \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n            \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n            \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n        }\n        output_names = [\"masks\", \"iou_predictions\"]\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            torch.onnx.export(\n                onnx_model,\n                tuple(dummy_inputs.values()),\n                onnx_model_path,\n                input_names=list(dummy_inputs.keys()),\n                output_names=output_names,\n                dynamic_axes=dynamic_axes,\n            )\n\n    ov_model = mo.convert_model(onnx_model_path, compress_to_fp16=True)\n    serialize(ov_model, str(ov_model_path))\n    print(\"sam_mask_predictor IR convetion done!\")\nelse:\n    ov_model = core.read_model(ov_model_path)\n    print(\"sam_mask_predictor already exists, skip convertion\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#reference https://docs.openvino.ai/2023.0/notebooks/237-segment-anything-with-output.html\nimport warnings\nfrom pathlib import Path\nimport torch\nfrom openvino.tools import mo\nfrom openvino.runtime import serialize, Core\nimport sys\n\nfrom segment_anything import sam_model_registry, SamPredictor\n# checkpoint = \"sam_vit_b_01ec64.pth\"\n# model_type = \"vit_b\"\ncheckpoint = \"sam_vit_b_01ec64.pth\"\nmodel_type = \"vit_h\"\n#sam = sam_model_registry[model_type](checkpoint=checkpoint)\ncore = Core()\n\nov_encoder_path = Path(\"sam_image_encoder.xml\")\nonnx_encoder_path = ov_encoder_path.with_suffix(\".onnx\")\nif not ov_encoder_path.exists():\n    print(\"sam_image_encoder.xml not found. pls run sam_model_convert\")\n    sys.exit(1)\nelse:\n    ov_encoder_model = core.read_model(ov_encoder_path)\nov_encoder = core.compile_model(ov_encoder_model, device_name=\"GPU\")\n\n#decoder\nfrom typing import Tuple\nfrom segment_anything.utils.amg import calculate_stability_score\n\nov_model_path = Path(\"sam_mask_predictor.xml\")\nif not ov_model_path.exists():\n    print(\"sam_mask_predictor.xml not found. pls run sam_model_convert\")\n    sys.exit(1)\nelse:\n    ov_model = core.read_model(ov_model_path)\nov_predictor = core.compile_model(ov_model, device_name=\"GPU\")\n\n\nimport numpy as np\nfrom copy import deepcopy\nfrom typing import Tuple\nfrom torchvision.transforms.functional import resize, to_pil_image\n\nclass ResizeLongestSide:\n    \"\"\"\n    Resizes images to longest side 'target_length', as well as provides\n    methods for resizing coordinates and boxes. Provides methods for\n    transforming numpy arrays.\n    \"\"\"\n\n    def __init__(self, target_length: int) -> None:\n        self.target_length = target_length\n\n    def apply_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array with shape HxWxC in uint8 format.\n        \"\"\"\n        target_size = self.get_preprocess_shape(image.shape[0], image.shape[1], self.target_length)\n        return np.array(resize(to_pil_image(image), target_size))\n\n    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array of length 2 in the final dimension. Requires the\n        original image size in (H, W) format.\n        \"\"\"\n        old_h, old_w = original_size\n        new_h, new_w = self.get_preprocess_shape(\n            original_size[0], original_size[1], self.target_length\n        )\n        coords = deepcopy(coords).astype(float)\n        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n        return coords\n\n    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n        \"\"\"\n        Expects a numpy array shape Bx4. Requires the original image size\n        in (H, W) format.\n        \"\"\"\n        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n        return boxes.reshape(-1, 4)\n\n    @staticmethod\n    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n        \"\"\"\n        Compute the output size given input size and target long side length.\n        \"\"\"\n        scale = long_side_length * 1.0 / max(oldh, oldw)\n        newh, neww = oldh * scale, oldw * scale\n        neww = int(neww + 0.5)\n        newh = int(newh + 0.5)\n        return (newh, neww)\n\n\nresizer = ResizeLongestSide(1024)\n\n\ndef preprocess_image(image: np.ndarray):\n    resized_image = resizer.apply_image(image)\n    resized_image = (resized_image.astype(np.float32) - [123.675, 116.28, 103.53]) / [58.395, 57.12, 57.375]\n    resized_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)).astype(np.float32), 0)\n\n    # Pad\n    h, w = resized_image.shape[-2:]\n    padh = 1024 - h\n    padw = 1024 - w\n    x = np.pad(resized_image, ((0, 0), (0, 0), (0, padh), (0, padw)))\n    return x\n\n\ndef postprocess_masks(masks: np.ndarray, orig_size):\n    size_before_pad = resizer.get_preprocess_shape(orig_size[0], orig_size[1], masks.shape[-1])\n    masks = masks[..., :int(size_before_pad[0]), :int(size_before_pad[1])]\n    masks = torch.nn.functional.interpolate(torch.from_numpy(masks), size=orig_size, mode=\"bilinear\", align_corners=False).numpy()\n    return masks\n\nimport cv2\n\n#interactive seg\nimport gradio as gr\nclass Segmenter:\n    def __init__(self, ov_encoder, ov_predictor):\n        self.encoder = ov_encoder\n        self.predictor = ov_predictor\n        self._img_embeddings = None\n\n    def set_image(self, img:np.ndarray):\n        if self._img_embeddings is not None:\n            del self._img_embeddings\n        preprocessed_image = preprocess_image(img)\n        encoding_results = self.encoder(preprocessed_image)\n        image_embeddings = encoding_results[ov_encoder.output(0)]\n        self._img_embeddings = image_embeddings\n        return img\n\n    def get_mask(self, points, img):\n        coord = np.array(points)\n        coord = np.concatenate([coord, np.array([[0,0]])], axis=0)\n        coord = coord[None, :, :]\n        label = np.concatenate([np.ones(len(points)), np.array([-1])], axis=0)[None, :].astype(np.float32)\n        coord = resizer.apply_coords(coord, img.shape[:2]).astype(np.float32)\n        if self._img_embeddings is None:\n            self.set_image(img)\n        inputs = {\n            \"image_embeddings\": self._img_embeddings,\n            \"point_coords\": coord,\n            \"point_labels\": label,\n        }\n\n        results = self.predictor(inputs)\n        masks = results[ov_predictor.output(0)]\n        masks = postprocess_masks(masks, img.shape[:-1])\n\n        masks = masks > 0.0\n        mask = masks[0]\n        mask = np.transpose(mask, (1, 2, 0))\n        return mask\n    \nsegmenter = Segmenter(ov_encoder, ov_predictor)\n\nfrom PIL import Image\nwith gr.Blocks() as demo:\n    with gr.Row():\n        input_img = gr.Image(label=\"Input\", type=\"numpy\").style(height=480, width=480)\n        output_img = gr.Image(label=\"Selected Segment\", type=\"numpy\").style(height=480, width=480)\n\n    def on_image_change(img):\n        segmenter.set_image(img)\n        return img\n        \n    def dump_mask(mask_image, mask, selected_val, rest_val, postfix):\n        mask_image_dump = mask_image\n        mask_image_dump[:] = rest_val\n        mask_image_dump[mask.squeeze(-1)] = selected_val\n  \n        dump_path = \"./sam_mask_result_\" + postfix + \".png\"\n        print(\"... Dumping mask img to \" + dump_path + \" ...\")\n        dump_image = Image.fromarray(mask_image_dump)\n        dump_image.save(dump_path)\n\n    def get_select_coords(img, evt: gr.SelectData):\n        pixels_in_queue = set()\n        h, w = img.shape[:2]\n        pixels_in_queue.add((evt.index[0], evt.index[1]))\n        out = img.copy()\n        while len(pixels_in_queue) > 0:\n            pixels = list(pixels_in_queue)\n            pixels_in_queue = set()\n            color = np.random.randint(0, 255, size=(1, 1, 3))\n            mask = segmenter.get_mask(pixels, img)\n            mask_image = out.copy()\n            \n            # dump mask\n            dump_mask(mask_image, mask, 0, 255, \"selected\")\n            dump_mask(mask_image, mask,  255, 0, \"reverted\")\n            \n            mask_image[mask.squeeze(-1)] = color\n            out = cv2.addWeighted(out.astype(np.float32), 0.7, mask_image.astype(np.float32), 0.3, 0.0)\n        out = out.astype(np.uint8)\n        return out\n\n    input_img.select(get_select_coords, [input_img], output_img)\n    input_img.upload(on_image_change, [input_img], [input_img])\n\ndemo.launch()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}